{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `datasets` submodule of `xarray_filters` provides data simulation capabilities.\n",
    "\n",
    "We wrap simulation functions from `scikit-learn` with our own code to return more flexible data structures.\n",
    "\n",
    "The goal is to make it easier to generate data for testing `elm`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:datasets:Cannot convert function make_biclusters. Some args of make_biclusters have no default value\n",
      "WARNING:datasets:Cannot convert function make_checkerboard. Some args of make_checkerboard have no default value\n",
      "WARNING:datasets:Cannot convert function make_sparse_coded_signal. Some args of make_sparse_coded_signal have no default value\n",
      "WARNING:datasets:Cannot convert function make_spd_matrix. Some args of make_spd_matrix have no default value\n"
     ]
    }
   ],
   "source": [
    "import datasets as ds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the at import time we are notified which functions from sklearn could not be converted. That is because we restrict ourselves to simulation functions from sklearn that\n",
    "\n",
    "- return a tuple `(X, y)` with a feature matrix `X` and a 1d vector of labels `y`;\n",
    "- can be called with default values alone\n",
    "\n",
    "That is to keep a section of code simple. Making our solution more general to address the two points above would be an unnecessary distraction at this stage.\n",
    "\n",
    "Note:\n",
    "\n",
    "- We can check that a function can be called with default values alone before we call the function. The warnings above are for the functions that fail that requirement.\n",
    "- However, we cannot check that a function returns the features/labels pair `(X, y)` without calling the function (not in Python). We will find those additional problematic functions in a [later section](#sec-okfuncs)(again, can be fixed, but it's a distraction now)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Showcasing the design and functionality"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A drop-in replacement of scikit-learn functionality"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `datasets` library was designed to provide drop-in replacements for the `sklearn.datasets.make_*` functions. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import sklearn.datasets as skd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[-1.53237475,  1.16966272,  0.26639974,  1.52058089],\n",
       "        [ 0.52829584, -1.70605142,  0.97125855, -0.09974296],\n",
       "        [ 0.52689704,  0.53680201, -0.85782025, -0.82878672],\n",
       "        [-0.75384103,  1.01255241, -0.22566169,  0.60560597],\n",
       "        [-0.13926741, -0.76073832,  0.73172656,  0.42070001],\n",
       "        [ 0.4773532 ,  1.02662418, -1.21804851, -0.92689922],\n",
       "        [ 0.63845131,  1.79302957, -1.9717922 , -1.37653776],\n",
       "        [-1.01950582, -0.19691761,  0.97293689,  1.32937436],\n",
       "        [-1.27077651,  1.28618119, -0.03709842,  1.157971  ],\n",
       "        [-0.16531167, -0.32750302,  0.39895146,  0.31186184],\n",
       "        [ 1.6606369 ,  1.37035939, -2.44127359, -2.50736   ],\n",
       "        [ 1.65162356, -1.59500861, -0.01431944, -1.52998076],\n",
       "        [ 1.35971085, -1.37196835,  0.03624686, -1.24038743],\n",
       "        [ 1.5988656 , -1.82660002,  0.21669448, -1.38904931],\n",
       "        [-2.77719001,  0.98620969,  1.40785555,  3.12517866],\n",
       "        [-0.11152424,  0.57879094, -0.3834474 , -0.05018274],\n",
       "        [ 0.49863053,  1.31471069, -1.47008215, -1.04717057],\n",
       "        [ 0.01422393,  0.1741922 , -0.1534751 , -0.07440832],\n",
       "        [ 1.06116573, -0.90425208, -0.10756108, -1.02228502],\n",
       "        [ 1.51731314, -1.04668764, -0.35474904, -1.54195779]]),\n",
       " array([1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0]))"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "skd.make_classification(n_samples=20, n_features=4, n_classes=2, random_state=0)  # sklearn function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[-1.53237475,  1.16966272,  0.26639974,  1.52058089],\n",
       "        [ 0.52829584, -1.70605142,  0.97125855, -0.09974296],\n",
       "        [ 0.52689704,  0.53680201, -0.85782025, -0.82878672],\n",
       "        [-0.75384103,  1.01255241, -0.22566169,  0.60560597],\n",
       "        [-0.13926741, -0.76073832,  0.73172656,  0.42070001],\n",
       "        [ 0.4773532 ,  1.02662418, -1.21804851, -0.92689922],\n",
       "        [ 0.63845131,  1.79302957, -1.9717922 , -1.37653776],\n",
       "        [-1.01950582, -0.19691761,  0.97293689,  1.32937436],\n",
       "        [-1.27077651,  1.28618119, -0.03709842,  1.157971  ],\n",
       "        [-0.16531167, -0.32750302,  0.39895146,  0.31186184],\n",
       "        [ 1.6606369 ,  1.37035939, -2.44127359, -2.50736   ],\n",
       "        [ 1.65162356, -1.59500861, -0.01431944, -1.52998076],\n",
       "        [ 1.35971085, -1.37196835,  0.03624686, -1.24038743],\n",
       "        [ 1.5988656 , -1.82660002,  0.21669448, -1.38904931],\n",
       "        [-2.77719001,  0.98620969,  1.40785555,  3.12517866],\n",
       "        [-0.11152424,  0.57879094, -0.3834474 , -0.05018274],\n",
       "        [ 0.49863053,  1.31471069, -1.47008215, -1.04717057],\n",
       "        [ 0.01422393,  0.1741922 , -0.1534751 , -0.07440832],\n",
       "        [ 1.06116573, -0.90425208, -0.10756108, -1.02228502],\n",
       "        [ 1.51731314, -1.04668764, -0.35474904, -1.54195779]]),\n",
       " array([1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0]))"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds.make_classification(n_samples=20, n_features=4, n_classes=2, random_state=0,  # sklearn args\n",
    "                       astype='array')                                           # new args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on function make_classification in module datasets:\n",
      "\n",
      "make_classification(n_samples=100, n_features=20, n_informative=2, n_redundant=2, n_repeated=0, n_classes=2, n_clusters_per_class=2, weights=None, flip_y=0.01, class_sep=1.0, hypercube=True, shift=0.0, scale=1.0, shuffle=True, random_state=None, *, astype='dataset', **kwargs)\n",
      "    Like sklearn.datasets.samples_generator.make_classification, but with added functionality.\n",
      "    \n",
      "    Parameters\n",
      "    ---------------------\n",
      "    Same parameters/arguments as sklearn.datasets.samples_generator.make_classification, in addition to the following\n",
      "    keyword-only arguments:\n",
      "    \n",
      "    astype: str\n",
      "        One of ('array', 'dataframe', 'dataset') or None to return an NpXyTransformer. See documentation\n",
      "        of NpXyTransformer.astype.\n",
      "        \n",
      "    **kwargs: dict\n",
      "        Optional arguments that depend on astype. See documentation of\n",
      "        NpXyTransformer.astype. \n",
      "    \n",
      "    See Also\n",
      "    --------\n",
      "    sklearn.datasets.samples_generator.make_classification\n",
      "    datasets.NpXyTransformer\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(ds.make_classification)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on function make_classification in module sklearn.datasets.samples_generator:\n",
      "\n",
      "make_classification(n_samples=100, n_features=20, n_informative=2, n_redundant=2, n_repeated=0, n_classes=2, n_clusters_per_class=2, weights=None, flip_y=0.01, class_sep=1.0, hypercube=True, shift=0.0, scale=1.0, shuffle=True, random_state=None)\n",
      "    Generate a random n-class classification problem.\n",
      "    \n",
      "    This initially creates clusters of points normally distributed (std=1)\n",
      "    about vertices of a `2 * class_sep`-sided hypercube, and assigns an equal\n",
      "    number of clusters to each class. It introduces interdependence between\n",
      "    these features and adds various types of further noise to the data.\n",
      "    \n",
      "    Prior to shuffling, `X` stacks a number of these primary \"informative\"\n",
      "    features, \"redundant\" linear combinations of these, \"repeated\" duplicates\n",
      "    of sampled features, and arbitrary noise for and remaining features.\n",
      "    \n",
      "    Read more in the :ref:`User Guide <sample_generators>`.\n",
      "    \n",
      "    Parameters\n",
      "    ----------\n",
      "    n_samples : int, optional (default=100)\n",
      "        The number of samples.\n",
      "    \n",
      "    n_features : int, optional (default=20)\n",
      "        The total number of features. These comprise `n_informative`\n",
      "        informative features, `n_redundant` redundant features, `n_repeated`\n",
      "        duplicated features and `n_features-n_informative-n_redundant-\n",
      "        n_repeated` useless features drawn at random.\n",
      "    \n",
      "    n_informative : int, optional (default=2)\n",
      "        The number of informative features. Each class is composed of a number\n",
      "        of gaussian clusters each located around the vertices of a hypercube\n",
      "        in a subspace of dimension `n_informative`. For each cluster,\n",
      "        informative features are drawn independently from  N(0, 1) and then\n",
      "        randomly linearly combined within each cluster in order to add\n",
      "        covariance. The clusters are then placed on the vertices of the\n",
      "        hypercube.\n",
      "    \n",
      "    n_redundant : int, optional (default=2)\n",
      "        The number of redundant features. These features are generated as\n",
      "        random linear combinations of the informative features.\n",
      "    \n",
      "    n_repeated : int, optional (default=0)\n",
      "        The number of duplicated features, drawn randomly from the informative\n",
      "        and the redundant features.\n",
      "    \n",
      "    n_classes : int, optional (default=2)\n",
      "        The number of classes (or labels) of the classification problem.\n",
      "    \n",
      "    n_clusters_per_class : int, optional (default=2)\n",
      "        The number of clusters per class.\n",
      "    \n",
      "    weights : list of floats or None (default=None)\n",
      "        The proportions of samples assigned to each class. If None, then\n",
      "        classes are balanced. Note that if `len(weights) == n_classes - 1`,\n",
      "        then the last class weight is automatically inferred.\n",
      "        More than `n_samples` samples may be returned if the sum of `weights`\n",
      "        exceeds 1.\n",
      "    \n",
      "    flip_y : float, optional (default=0.01)\n",
      "        The fraction of samples whose class are randomly exchanged.\n",
      "    \n",
      "    class_sep : float, optional (default=1.0)\n",
      "        The factor multiplying the hypercube dimension.\n",
      "    \n",
      "    hypercube : boolean, optional (default=True)\n",
      "        If True, the clusters are put on the vertices of a hypercube. If\n",
      "        False, the clusters are put on the vertices of a random polytope.\n",
      "    \n",
      "    shift : float, array of shape [n_features] or None, optional (default=0.0)\n",
      "        Shift features by the specified value. If None, then features\n",
      "        are shifted by a random value drawn in [-class_sep, class_sep].\n",
      "    \n",
      "    scale : float, array of shape [n_features] or None, optional (default=1.0)\n",
      "        Multiply features by the specified value. If None, then features\n",
      "        are scaled by a random value drawn in [1, 100]. Note that scaling\n",
      "        happens after shifting.\n",
      "    \n",
      "    shuffle : boolean, optional (default=True)\n",
      "        Shuffle the samples and the features.\n",
      "    \n",
      "    random_state : int, RandomState instance or None, optional (default=None)\n",
      "        If int, random_state is the seed used by the random number generator;\n",
      "        If RandomState instance, random_state is the random number generator;\n",
      "        If None, the random number generator is the RandomState instance used\n",
      "        by `np.random`.\n",
      "    \n",
      "    Returns\n",
      "    -------\n",
      "    X : array of shape [n_samples, n_features]\n",
      "        The generated samples.\n",
      "    \n",
      "    y : array of shape [n_samples]\n",
      "        The integer labels for class membership of each sample.\n",
      "    \n",
      "    Notes\n",
      "    -----\n",
      "    The algorithm is adapted from Guyon [1] and was designed to generate\n",
      "    the \"Madelon\" dataset.\n",
      "    \n",
      "    References\n",
      "    ----------\n",
      "    .. [1] I. Guyon, \"Design of experiments for the NIPS 2003 variable\n",
      "           selection benchmark\", 2003.\n",
      "    \n",
      "    See also\n",
      "    --------\n",
      "    make_blobs: simplified variant\n",
      "    make_multilabel_classification: unrelated generator for multilabel tasks\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(skd.make_classification)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### An extension of scikit-learn functionality"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also provide postprocessing functionality on top of the `scikit-learn` routines via additional keywords (`astype` and `feature_shape` below)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[[-1.53237475,  1.16966272,  0.26639974,  1.52058089],\n",
       "         [ 0.52829584, -1.70605142,  0.97125855, -0.09974296],\n",
       "         [ 0.52689704,  0.53680201, -0.85782025, -0.82878672],\n",
       "         [-0.75384103,  1.01255241, -0.22566169,  0.60560597],\n",
       "         [-0.13926741, -0.76073832,  0.73172656,  0.42070001],\n",
       "         [ 0.4773532 ,  1.02662418, -1.21804851, -0.92689922],\n",
       "         [ 0.63845131,  1.79302957, -1.9717922 , -1.37653776],\n",
       "         [-1.01950582, -0.19691761,  0.97293689,  1.32937436],\n",
       "         [-1.27077651,  1.28618119, -0.03709842,  1.157971  ],\n",
       "         [-0.16531167, -0.32750302,  0.39895146,  0.31186184]],\n",
       " \n",
       "        [[ 1.6606369 ,  1.37035939, -2.44127359, -2.50736   ],\n",
       "         [ 1.65162356, -1.59500861, -0.01431944, -1.52998076],\n",
       "         [ 1.35971085, -1.37196835,  0.03624686, -1.24038743],\n",
       "         [ 1.5988656 , -1.82660002,  0.21669448, -1.38904931],\n",
       "         [-2.77719001,  0.98620969,  1.40785555,  3.12517866],\n",
       "         [-0.11152424,  0.57879094, -0.3834474 , -0.05018274],\n",
       "         [ 0.49863053,  1.31471069, -1.47008215, -1.04717057],\n",
       "         [ 0.01422393,  0.1741922 , -0.1534751 , -0.07440832],\n",
       "         [ 1.06116573, -0.90425208, -0.10756108, -1.02228502],\n",
       "         [ 1.51731314, -1.04668764, -0.35474904, -1.54195779]]]),\n",
       " array([1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0]))"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds.make_classification(n_samples=20, n_features=4, n_classes=2, random_state=0,  # sklearn args\n",
    "                       astype='array', feature_shape=(2,10,4))                   # new args"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also convert to `xarray.Dataset` (or other types, like `pandas.DataFrame`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<xarray.Dataset>\n",
       "Dimensions:  (dim_0: 20)\n",
       "Dimensions without coordinates: dim_0\n",
       "Data variables:\n",
       "    X0       (dim_0) float64 -1.532 0.5283 0.5269 -0.7538 -0.1393 0.4774 ...\n",
       "    X1       (dim_0) float64 1.17 -1.706 0.5368 1.013 -0.7607 1.027 1.793 ...\n",
       "    X2       (dim_0) float64 0.2664 0.9713 -0.8578 -0.2257 0.7317 -1.218 ...\n",
       "    X3       (dim_0) float64 1.521 -0.09974 -0.8288 0.6056 0.4207 -0.9269 ...\n",
       "    y        (dim_0) int64 1 1 1 1 1 0 0 1 1 1 0 0 0 0 1 1 0 0 0 0"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dst = ds.make_classification(n_samples=20, n_features=4, n_classes=2, random_state=0,  # sklearn args\n",
    "                            astype='dataset')                                          # new args\n",
    "dst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<xarray.DataArray 'y' (dim_0: 20)>\n",
       "array([1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0])\n",
       "Dimensions without coordinates: dim_0"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dst.y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<xarray.Dataset>\n",
       "Dimensions:  (horizontal: 4, vertical: 5)\n",
       "Dimensions without coordinates: horizontal, vertical\n",
       "Data variables:\n",
       "    X0       (horizontal, vertical) float64 -1.532 0.5283 0.5269 -0.7538 ...\n",
       "    X1       (horizontal, vertical) float64 1.17 -1.706 0.5368 1.013 -0.7607 ...\n",
       "    X2       (horizontal, vertical) float64 0.2664 0.9713 -0.8578 -0.2257 ...\n",
       "    X3       (horizontal, vertical) float64 1.521 -0.09974 -0.8288 0.6056 ...\n",
       "    y        (horizontal, vertical) int64 1 1 1 1 1 0 0 1 1 1 0 0 0 0 1 1 0 ..."
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dst = ds.make_classification(n_samples=20, n_features=4, n_classes=2, random_state=0,  # sklearn args\n",
    "                             astype='dataset', dims=('horizontal','vertical'), shape=(4,5))            # new args\n",
    "dst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<xarray.DataArray 'y' (horizontal: 4, vertical: 5)>\n",
       "array([[1, 1, 1, 1, 1],\n",
       "       [0, 0, 1, 1, 1],\n",
       "       [0, 0, 0, 0, 1],\n",
       "       [1, 0, 0, 0, 0]])\n",
       "Dimensions without coordinates: horizontal, vertical"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dst.y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<xarray.Dataset>\n",
       "Dimensions:     (horizontal: 4, vertical: 5)\n",
       "Coordinates:\n",
       "  * horizontal  (horizontal) <U1 'a' 'b' 'c' 'd'\n",
       "  * vertical    (vertical) <U1 'e' 'f' 'g' 'h' 'i'\n",
       "Data variables:\n",
       "    feat_0      (horizontal, vertical) float64 -1.532 0.5283 0.5269 -0.7538 ...\n",
       "    feat_1      (horizontal, vertical) float64 1.17 -1.706 0.5368 1.013 ...\n",
       "    feat_2      (horizontal, vertical) float64 0.2664 0.9713 -0.8578 -0.2257 ...\n",
       "    feat_3      (horizontal, vertical) float64 1.521 -0.09974 -0.8288 0.6056 ...\n",
       "    LABEL       (horizontal, vertical) int64 1 1 1 1 1 0 0 1 1 1 0 0 0 0 1 1 ...\n",
       "Attributes:\n",
       "    metadata1:  super important"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dst = ds.make_classification(n_samples=20, n_features=4, n_classes=2, random_state=0,  # sklearn args\n",
    "                             astype='dataset', dims=('horizontal','vertical'), shape=(4,5),\n",
    "                             coords=(list('abcd'), list('efghi')),\n",
    "                             xnames=['feat_{:d}'.format(n) for n in range(4)],\n",
    "                             yname='LABEL', attrs={'metadata1': 'super important'})  \n",
    "dst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<xarray.DataArray 'LABEL' (horizontal: 4, vertical: 5)>\n",
       "array([[1, 1, 1, 1, 1],\n",
       "       [0, 0, 1, 1, 1],\n",
       "       [0, 0, 0, 0, 1],\n",
       "       [1, 0, 0, 0, 0]])\n",
       "Coordinates:\n",
       "  * horizontal  (horizontal) <U1 'a' 'b' 'c' 'd'\n",
       "  * vertical    (vertical) <U1 'e' 'f' 'g' 'h' 'i'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dst.LABEL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Which simulation functions can be used right now?\n",
    "<a id='sec-okfuncs'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ds_make_funcs = [f for f in dir(ds) if f.startswith('make_')]  # all make_* functions in xarray_filters/datasets.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All of the functions above work with defaults only.\n",
    "\n",
    "But some of them do not return a tuple `(X, y)` where X is a feature matrix and y is a 1d vector of labels.\n",
    "\n",
    "We will find which ones now (see the `bad` list below)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ERROR: Function make_low_rank_matrix must return a tuple of 2 elements\n",
      "ERROR: Y must have dimension 1.\n",
      "ERROR: Function make_sparse_spd_matrix must return a tuple of 2 elements\n"
     ]
    }
   ],
   "source": [
    "good = []  # to store the make_* functions that return a features/labels pair (X, y)\n",
    "bad = []   # to store the make_* functions that do _not_ return a features/labels pair (X, y)\n",
    "\n",
    "for f in ds_make_funcs:\n",
    "    try:\n",
    "        simdata = ds.__getattribute__(f)(astype='array')\n",
    "    except ValueError as e:\n",
    "        print('ERROR: {}'.format(str(e)))\n",
    "        bad.append(f)\n",
    "    else:\n",
    "        good.append(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see the problematic functions in the error messages above. Also listed here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['make_low_rank_matrix',\n",
       " 'make_multilabel_classification',\n",
       " 'make_sparse_spd_matrix']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And here are the functions we can use without a problem with the current implementation.\n",
    "\n",
    "Again, we can make it more general, but I'd recommend doing that after we pin down the whole API and tests for the functions that work with the simpler code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['make_blobs',\n",
       " 'make_circles',\n",
       " 'make_classification',\n",
       " 'make_friedman1',\n",
       " 'make_friedman2',\n",
       " 'make_friedman3',\n",
       " 'make_gaussian_quantiles',\n",
       " 'make_hastie_10_2',\n",
       " 'make_moons',\n",
       " 'make_regression',\n",
       " 'make_s_curve',\n",
       " 'make_sparse_uncorrelated',\n",
       " 'make_swiss_roll']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "good"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementation details\n",
    "\n",
    "The central functionality here is implemented in the following two objects:\n",
    "\n",
    "- The `NpXyTransformer` class that has multiple `to_*` methods (`to_dataset`, `to_dataframe`, `to_array`, etc.). Adding different postprocessing routines can be done by adding a new `NpXyTransformer.to_*` method with the appropriate code and documentation.\n",
    "- A `_make_base` function that takes as input a `sklearn.datasets._make_*` function (like `make_classification`) and creates a new \"version\" of it under the `datasets` namespace, with useful signature, docs and extended functionality.\n",
    "\n",
    "It's easier to see with an example. Let's construct the same data with the \"direct\" approach (using the keyword `astype` inside the `make_*` function) and the step-by-step approach (which is what the direct approach does under the hood)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X1, y1 = ds.make_classification(n_samples=20, n_features=4, n_classes=2, random_state=0,  # sklearn args\n",
    "                                astype='array', feature_shape=(2,10,4))                   # new args    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Xyt = ds.make_classification(n_samples=20, n_features=4, n_classes=2, random_state=0,  # sklearn args\n",
    "                             astype=None)\n",
    "X2, y2 = Xyt.to_array(feature_shape=(2,10,4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "np.allclose(X1, X2)  # floating-point data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.alltrue(y1 == y2)  # integer data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on function astype in module datasets:\n",
      "\n",
      "astype(self, to_type, **kwargs)\n",
      "    Convert to given type.\n",
      "    \n",
      "    self.astype(f, **kwargs) calls self.to_f(**kwargs)\n",
      "    \n",
      "    Valid types are in NpXyTransformer.accepted_types.\n",
      "    \n",
      "    See Also\n",
      "    --------\n",
      "    \n",
      "    NpXyTransformer.to_dataset\n",
      "    NpXyTransformer.to_array\n",
      "    NpXyTransformer.to_dataframe\n",
      "    NpXyTransformer.to_*\n",
      "    etc...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(ds.NpXyTransformer.astype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ds.NpXyTransformer.astype??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on function to_array in module datasets:\n",
      "\n",
      "to_array(self, feature_shape=None)\n",
      "    Return X, y NumPy arrays with given shape\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(ds.NpXyTransformer.to_array)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This design allows us to implement any data transformations we want by just creating new `to_*` methods under `NpXyTransformer`, while still enjoying:\n",
    "\n",
    "- All the work (code and docs) done in sklearn\n",
    "- Argument checking, docs for each transformation in its own method, easier to inspect than `**kwargs` with lots of `if/else` checks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For recap, here is the full \"low-level path\" to a new `make_classification` function and using it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[-1.53237475,  1.16966272,  0.26639974,  1.52058089],\n",
       "        [ 0.52829584, -1.70605142,  0.97125855, -0.09974296],\n",
       "        [ 0.52689704,  0.53680201, -0.85782025, -0.82878672],\n",
       "        [-0.75384103,  1.01255241, -0.22566169,  0.60560597],\n",
       "        [-0.13926741, -0.76073832,  0.73172656,  0.42070001],\n",
       "        [ 0.4773532 ,  1.02662418, -1.21804851, -0.92689922],\n",
       "        [ 0.63845131,  1.79302957, -1.9717922 , -1.37653776],\n",
       "        [-1.01950582, -0.19691761,  0.97293689,  1.32937436],\n",
       "        [-1.27077651,  1.28618119, -0.03709842,  1.157971  ],\n",
       "        [-0.16531167, -0.32750302,  0.39895146,  0.31186184],\n",
       "        [ 1.6606369 ,  1.37035939, -2.44127359, -2.50736   ],\n",
       "        [ 1.65162356, -1.59500861, -0.01431944, -1.52998076],\n",
       "        [ 1.35971085, -1.37196835,  0.03624686, -1.24038743],\n",
       "        [ 1.5988656 , -1.82660002,  0.21669448, -1.38904931],\n",
       "        [-2.77719001,  0.98620969,  1.40785555,  3.12517866],\n",
       "        [-0.11152424,  0.57879094, -0.3834474 , -0.05018274],\n",
       "        [ 0.49863053,  1.31471069, -1.47008215, -1.04717057],\n",
       "        [ 0.01422393,  0.1741922 , -0.1534751 , -0.07440832],\n",
       "        [ 1.06116573, -0.90425208, -0.10756108, -1.02228502],\n",
       "        [ 1.51731314, -1.04668764, -0.35474904, -1.54195779]]),\n",
       " array([1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0]))"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_classification = ds._make_base(skd.make_classification)\n",
    "Xyt = my_classification(n_samples=20, n_features=4, n_classes=2, random_state=0, astype=None)\n",
    "X, y = Xyt.to_array()\n",
    "X, y\n",
    "\n",
    "# same as\n",
    "# ds.make_classification(n_samples=20, n_features=4, n_classes=2, random_state=0, astype='array')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on function make_classification in module datasets:\n",
      "\n",
      "make_classification(n_samples=100, n_features=20, n_informative=2, n_redundant=2, n_repeated=0, n_classes=2, n_clusters_per_class=2, weights=None, flip_y=0.01, class_sep=1.0, hypercube=True, shift=0.0, scale=1.0, shuffle=True, random_state=None, *, astype='dataset', **kwargs)\n",
      "    Like sklearn.datasets.samples_generator.make_classification, but with added functionality.\n",
      "    \n",
      "    Parameters\n",
      "    ---------------------\n",
      "    Same parameters/arguments as sklearn.datasets.samples_generator.make_classification, in addition to the following\n",
      "    keyword-only arguments:\n",
      "    \n",
      "    astype: str\n",
      "        One of ('array', 'dataframe', 'dataset') or None to return an NpXyTransformer. See documentation\n",
      "        of NpXyTransformer.astype.\n",
      "        \n",
      "    **kwargs: dict\n",
      "        Optional arguments that depend on astype. See documentation of\n",
      "        NpXyTransformer.astype. \n",
      "    \n",
      "    See Also\n",
      "    --------\n",
      "    sklearn.datasets.samples_generator.make_classification\n",
      "    datasets.NpXyTransformer\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(my_classification)  # signature/docstring build automatically"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:py36]",
   "language": "python",
   "name": "conda-env-py36-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
